{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import konfuzio_sdk\n",
    "from konfuzio_sdk.data import Project\n",
    "from konfuzio_sdk.tokenizer.regex import WhitespaceTokenizer as KWT\n",
    "from nltk.tokenize import word_tokenize, NLTKWordTokenizer\n",
    "from nltk.tokenize import WhitespaceTokenizer as WT\n",
    "import os\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from PIL import Image \n",
    "from PIL import ImageDraw, ImageFont\n",
    "import PIL \n",
    "import json\n",
    "\n",
    "def get_word_bboxes(document):\n",
    "\n",
    "    raw_document = deepcopy(document)  # get Document with no Annotations\n",
    "    tokenizer = KWT()\n",
    "    tk_document = tokenizer(raw_document)\n",
    "    tk_document.get_images(update=True)\n",
    "\n",
    "    return tk_document\n",
    "\n",
    "tk_doc = get_word_bboxes(document)\n",
    "\n",
    "def process_annotated_document(document):\n",
    "    dataset_column_dictionary = {\n",
    "        'annotation_id': [],\n",
    "        'page_index': [],\n",
    "        'line_number': [],\n",
    "        'top': [],\n",
    "        'bottom': [],\n",
    "        'x0': [],\n",
    "        'y0': [],\n",
    "        'x1': [],\n",
    "        'y1': [],\n",
    "        'label': [],\n",
    "        'label_set': [],\n",
    "        'start_offset': [],\n",
    "        'end_offset': [],\n",
    "        'offset_string': [],\n",
    "        'file_name': [],\n",
    "        'file_path': []\n",
    "    }\n",
    "\n",
    "    annotations = document.get_annotations()\n",
    "    file_paths_array = []\n",
    "    for idx, _ in enumerate(annotations):\n",
    "        \n",
    "        if annotations[idx].is_correct != True:\n",
    "            continue\n",
    "        \n",
    "        nr_bboxes_in_annotations = len(annotations[idx].bboxes)\n",
    "\n",
    "        annotation_id = annotations[idx].id_\n",
    "\n",
    "        for bnr in range(nr_bboxes_in_annotations):\n",
    "\n",
    "            bbox = annotations[idx].bboxes[bnr]\n",
    "\n",
    "            page = document.pages()[bbox['page_index']]\n",
    "            line_number = bbox['line_number']\n",
    "            up_factor_x = page.get_image().width / page.width  # image > page, thus up_factor_x > 1\n",
    "            up_factor_y = page.get_image().height / page.height # image > page, thus up_factor_y > 1\n",
    "\n",
    "            page_index = bbox['page_index'] + 1\n",
    "            top = bbox['top']\n",
    "            bottom = bbox['bottom']\n",
    "\n",
    "            x0 = bbox['x0'] * up_factor_x\n",
    "            y0 = bbox['top'] * up_factor_y\n",
    "            x1 = bbox['x1'] * up_factor_x\n",
    "            y1 = bbox['bottom'] * up_factor_y\n",
    "\n",
    "            #all_annotation_parts_x0 = annotations[idx].x0 * up_factor_x\n",
    "            #all_annotation_parts_y0 = annotations[idx].top * up_factor_y\n",
    "            #all_annotation_parts_x1 = annotations[idx].x1 * up_factor_x\n",
    "            #all_annotation_parts_y1 = annotations[idx].bottom * up_factor_y\n",
    "\n",
    "            label = annotations[idx].label.name_clean\n",
    "            label_set = annotations[idx].label_set.name_clean\n",
    "    \n",
    "            start_offset = bbox['start_offset']\n",
    "            end_offset = bbox['end_offset']\n",
    "            offset_string = bbox['offset_string']\n",
    "            file_name = \"page_\" + str(page_index) + \".png\"\n",
    "            file_path = os.getcwd()  + '/' + 'data_' + str(project_id) + '/documents/' + str(annotations[idx].document.id_) + \"/\" + file_name\n",
    "\n",
    "            file_paths_array.append(file_path)\n",
    "\n",
    "            dataset_column_dictionary['annotation_id'].append(annotation_id)\n",
    "            dataset_column_dictionary['page_index'].append(page_index)\n",
    "            dataset_column_dictionary['line_number'].append(line_number)\n",
    "\n",
    "            dataset_column_dictionary['top'].append(top)\n",
    "            dataset_column_dictionary['bottom'].append(bottom)\n",
    "            dataset_column_dictionary['x0'].append(x0)\n",
    "            dataset_column_dictionary['y0'].append(y0)\n",
    "            dataset_column_dictionary['x1'].append(x1)\n",
    "            dataset_column_dictionary['y1'].append(y1)\n",
    "\n",
    "            #dataset_column_dictionary['all_annotation_parts_x0'].append(all_annotation_parts_x0)\n",
    "            #dataset_column_dictionary['all_annotation_parts_y0'].append(all_annotation_parts_y0)\n",
    "            #dataset_column_dictionary['all_annotation_parts_x1'].append(all_annotation_parts_x1)\n",
    "            #dataset_column_dictionary['all_annotation_parts_y1'].append(all_annotation_parts_y1)\n",
    "\n",
    "            dataset_column_dictionary['label'].append(label)\n",
    "            dataset_column_dictionary['label_set'].append(label_set)\n",
    "            dataset_column_dictionary['start_offset'].append(start_offset)\n",
    "            dataset_column_dictionary['end_offset'].append(end_offset)\n",
    "            dataset_column_dictionary['offset_string'].append(offset_string)\n",
    "            dataset_column_dictionary['file_name'].append(file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                dataset_column_dictionary['file_path'].append(file_path)\n",
    "            else:\n",
    "                dataset_column_dictionary['file_path'].append(\"NA\")\n",
    "\n",
    "    return dataset_column_dictionary, file_paths_array\n",
    "\n",
    "def process_word_bboxes_document(document):\n",
    "    \n",
    "    dataset_column_dictionary = {\n",
    "        'annotation_id': [],\n",
    "        'page_index': [],\n",
    "        'line_number': [],\n",
    "        'offset_string': [],\n",
    "        'offset_string_original': [],\n",
    "        'top': [],\n",
    "        'bottom': [],\n",
    "        'x0': [],\n",
    "        'y0': [],\n",
    "        'x1': [],\n",
    "        'y1': [],\n",
    "        'start_offset': [],\n",
    "        'end_offset': [],\n",
    "        'file_name': [],\n",
    "        'file_path': []\n",
    "    }\n",
    "\n",
    "    annotations = document.get_annotations()\n",
    "    file_paths_array = []\n",
    "    for idx, _ in enumerate(annotations):\n",
    "        nr_bboxes_in_annotations = len(annotations[idx].bboxes)\n",
    "\n",
    "        annotation_id = annotations[idx].id_\n",
    "\n",
    "        for bnr in range(nr_bboxes_in_annotations):\n",
    "\n",
    "            bbox = annotations[idx].bboxes[bnr]\n",
    "            line_number = bbox['line_number']\n",
    "\n",
    "            page = document.pages()[bbox['page_index']]\n",
    "            up_factor_x = page.get_image().width / page.width  # image > page, thus up_factor_x > 1\n",
    "            up_factor_y = page.get_image().height / page.height # image > page, thus up_factor_y > 1\n",
    "\n",
    "            page_index = bbox['page_index'] + 1\n",
    "            top = bbox['top']\n",
    "            bottom = bbox['bottom']\n",
    "            x0 = bbox['x0'] * up_factor_x\n",
    "            y0 = bbox['top'] * up_factor_y\n",
    "            x1 = bbox['x1'] * up_factor_x\n",
    "            y1 = bbox['bottom'] * up_factor_y\n",
    "            start_offset = bbox['start_offset']\n",
    "            end_offset = bbox['end_offset']\n",
    "            offset_string = bbox['offset_string']\n",
    "            file_name = \"page_\" + str(page_index) + \".png\"\n",
    "            file_path =  os.getcwd() + '/' + 'data_' + str(project_id) + '/documents/' + str(annotations[idx].document.copy_of_id) + \"/\" + file_name\n",
    "            file_paths_array.append(file_path)\n",
    "\n",
    "            dataset_column_dictionary['page_index'].append(page_index)\n",
    "            dataset_column_dictionary['annotation_id'].append(annotation_id)\n",
    "            dataset_column_dictionary['start_offset'].append(start_offset)\n",
    "            dataset_column_dictionary['end_offset'].append(end_offset)\n",
    "            dataset_column_dictionary['offset_string'].append(offset_string)\n",
    "            dataset_column_dictionary['offset_string_original'].append(offset_string)\n",
    "            dataset_column_dictionary['top'].append(top)\n",
    "            dataset_column_dictionary['bottom'].append(bottom)\n",
    "            dataset_column_dictionary['x0'].append(x0)\n",
    "            dataset_column_dictionary['y0'].append(y0)\n",
    "            dataset_column_dictionary['x1'].append(x1)\n",
    "            dataset_column_dictionary['y1'].append(y1)\n",
    "            dataset_column_dictionary['line_number'].append(line_number)\n",
    "            dataset_column_dictionary['file_name'].append(file_name)\n",
    "            if os.path.exists(file_path):\n",
    "                dataset_column_dictionary['file_path'].append(file_path)\n",
    "            else:\n",
    "                dataset_column_dictionary['file_path'].append(\"NA\")\n",
    "\n",
    "    return dataset_column_dictionary, file_paths_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_bounding_boxes(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge bounding boxes based on 'annotation_id' and 'annotation_id' + 'line_number'.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: DataFrame containing the 'bbox', 'annotation_id', and 'line_number' columns\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with the added 'merged_bbboxes' and 'merged_bbboxes_by_line' columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- First Round: Merge based on annotation_id only ---\n",
    "    \n",
    "    def merge_bboxes(group):\n",
    "        \"\"\"\n",
    "        Merge bounding boxes for a group of rows.\n",
    "        \"\"\"\n",
    "        # Ensure that the 'bbox' column contains strings\n",
    "        bboxes_str = group['bbox'].astype(str)\n",
    "        \n",
    "        # Extract bounding boxes from the 'bbox' column\n",
    "        bboxes = bboxes_str.str.strip(\"[]\").str.split(\",\", expand=True).astype(float)\n",
    "        \n",
    "        # Compute the merged bounding box\n",
    "        merged_bbox = [\n",
    "            int(bboxes[0].min()),  # x0\n",
    "            int(bboxes[1].min()),  # y0\n",
    "            int(bboxes[2].max()),  # x1\n",
    "            int(bboxes[3].max())   # y1\n",
    "        ]\n",
    "        \n",
    "        return merged_bbox\n",
    "    \n",
    "    # Group by 'annotation_id' and apply the merge_bboxes function\n",
    "    merged_bboxes_dict = dataframe.groupby('annotation_id').apply(merge_bboxes).to_dict()\n",
    "\n",
    "    # Map the merged bounding boxes to the original dataframe using 'annotation_id' as the key\n",
    "    dataframe['merged_bbboxes'] = dataframe['annotation_id'].map(merged_bboxes_dict)\n",
    "    \n",
    "    # For rows without a related annotation, copy the bounding box from 'bbox'\n",
    "    dataframe['merged_bbboxes'].fillna(dataframe['bbox'], inplace=True)\n",
    "    \n",
    "    # --- Second Round: Merge based on annotation_id and line_number ---\n",
    "    \n",
    "    # Group by 'annotation_id' and 'line_number' and apply the merge_bboxes function\n",
    "    key = dataframe['annotation_id'].astype(str) + \"_\" + dataframe['line_number'].astype(str)\n",
    "    merged_bboxes_by_line_dict = dataframe.groupby(['annotation_id', 'line_number']).apply(merge_bboxes).to_dict()\n",
    "\n",
    "    # Convert multi-index dict keys to concatenated string keys with underscore separator\n",
    "    merged_bboxes_by_line_dict = {str(k[0]) + \"_\" + str(k[1]): v for k, v in merged_bboxes_by_line_dict.items()}\n",
    "\n",
    "    # Map the merged bounding boxes to a new column in the dataframe using the unique key\n",
    "    dataframe['merged_bbboxes_by_line'] = key.map(merged_bboxes_by_line_dict)\n",
    "    \n",
    "    # For rows without a merged bounding box based on both annotation_id and line_number, copy the bounding box from 'bbox'\n",
    "    dataframe['merged_bbboxes_by_line'].fillna(dataframe['bbox'], inplace=True)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def get_dataframes_from_doc(document: konfuzio_sdk.data.Document) -> konfuzio_sdk.data.Document:\n",
    "    \n",
    "    doc_with_word_level_bboxes = get_word_bboxes(document)\n",
    "    word_boxes_dataset_column_dictionary, _ = process_word_bboxes_document(doc_with_word_level_bboxes)\n",
    "    no_label_annotations_df = pd.DataFrame(word_boxes_dataset_column_dictionary)\n",
    "\n",
    "    dataset_column_dictionary, _ = process_annotated_document(document)\n",
    "    label_annotations_df = pd.DataFrame(dataset_column_dictionary)\n",
    "\n",
    "    return no_label_annotations_df, label_annotations_df\n",
    "\n",
    "def transfer_annotations(df1: pd.DataFrame, df2: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    no_label_annotations_df = df1.copy()\n",
    "    label_annotations_df = df2.copy()\n",
    "\n",
    "    # Initialize the new columns 'label' and 'label_set' in df1 with default values\n",
    "    no_label_annotations_df['label'] = 'NO_LABEL'\n",
    "    no_label_annotations_df['label_set'] = 'NO_LABEL_SET'\n",
    "\n",
    "    # Iterate through the rows of df1 and df2 and check for matching spans\n",
    "    for index1, row1 in no_label_annotations_df.iterrows():\n",
    "        for index2, row2 in label_annotations_df.iterrows():\n",
    "            if row1['start_offset'] >= row2['start_offset'] and row1['end_offset'] <= row2['end_offset']:\n",
    "                no_label_annotations_df.at[index1, 'label'] = row2['label']\n",
    "                no_label_annotations_df.at[index1, 'label_set'] = row2['label_set']\n",
    "                no_label_annotations_df.at[index1, 'annotation_id'] = row2['annotation_id']\n",
    "                break  # break the inner loop once a match is found\n",
    "    \n",
    "    full_df = no_label_annotations_df\n",
    "\n",
    "    full_df['bbox'] = list(map(list, zip(full_df['x0'].astype(int), full_df['y0'].astype(int), full_df['x1'].astype(int), full_df['y1'].astype(int))))\n",
    "\n",
    "    return full_df\n",
    "\n",
    "    def process_all_documents(documents):\n",
    "    \"\"\"\n",
    "    Process all documents and organize the results in a structured dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    - documents: List of documents to be processed.\n",
    "\n",
    "    Returns:\n",
    "    - tabular_document_datasets_dict: A dictionary where the key is the document's name and \n",
    "      the value is another dictionary containing 'pages' and 'document_text'.\n",
    "    \"\"\"\n",
    "    \n",
    "    tabular_document_datasets_dict = {}\n",
    "\n",
    "    for document in documents:\n",
    "        # Get dataframes from the document\n",
    "        no_label_annotations_df, label_annotations_df = get_dataframes_from_doc(document)\n",
    "        \n",
    "        # Transfer annotations and merge bounding boxes\n",
    "        full_df = transfer_annotations(no_label_annotations_df, label_annotations_df)\n",
    "        full_df = merge_bounding_boxes(full_df)\n",
    "        \n",
    "        # Split up full_df into chunks corresponding to the pages\n",
    "        pages_list = []\n",
    "        for _, group in full_df.groupby('file_name'):\n",
    "            pages_list.append(group)\n",
    "        \n",
    "        # Store the data in the main dictionary\n",
    "        tabular_document_datasets_dict[document.name] = {\n",
    "            'pages': pages_list,\n",
    "            'document_text': document.text\n",
    "        }\n",
    "\n",
    "    return tabular_document_datasets_dict\n",
    "\n",
    "\n",
    "\n",
    "def save_to_json(data_dict, file_path):\n",
    "    \"\"\"\n",
    "    Save the provided dictionary to a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dict: Dictionary to be saved. It contains lists of DataFrames.\n",
    "    - file_path: Path to the file where the dictionary will be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert DataFrames to a serializable format (dictionary)\n",
    "    serializable_dict = {}\n",
    "    for doc_key, doc_value in data_dict.items():\n",
    "        pages_list_serializable = [page_df.to_dict(orient='records') for page_df in doc_value['pages']]\n",
    "        serializable_dict[doc_key] = {\n",
    "            'pages': pages_list_serializable,\n",
    "            'document_text': doc_value['document_text']\n",
    "        }\n",
    "\n",
    "    # Save the serializable dictionary to a JSON file\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(serializable_dict, file)\n",
    "\n",
    "def load_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Load a dictionary from a JSON file and convert serialized data back to DataFrames.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary with lists of DataFrames.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the dictionary from the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        loaded_dict = json.load(file)\n",
    "\n",
    "    # Convert serialized data back to DataFrames\n",
    "    dataframes_dict = {}\n",
    "    for doc_key, doc_value in loaded_dict.items():\n",
    "        pages_list_df = [pd.DataFrame(page_data) for page_data in doc_value['pages']]\n",
    "        dataframes_dict[doc_key] = {\n",
    "            'pages': pages_list_df,\n",
    "            'document_text': doc_value['document_text']\n",
    "        }\n",
    "\n",
    "    return dataframes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# These functions allow saving the dictionary to a JSON file and loading it back, with proper conversions.\n",
    "\n",
    "\n",
    "# Needs implementation:\n",
    "\n",
    "# Pickle:\n",
    "# Python's native serialization method.\n",
    "# Allows storing of Python objects without needing to convert them to another data structure.\n",
    "# Quick to save and load.\n",
    "# Caution: Loading pickled data from untrusted sources can be a security risk.\n",
    "\n",
    "# HDF5 (with Pandas):\n",
    "# A hierarchical data format suitable for storing large datasets.\n",
    "# Pandas has built-in support for HDF5 through the HDFStore class.\n",
    "# Efficiently stores large DataFrames and supports complex querying.\n",
    "\n",
    "# SQLite (with Pandas):\n",
    "# Lightweight disk-based database.\n",
    "# Pandas can save DataFrames directly to SQLite with the to_sql method and read with read_sql_query.\n",
    "# Useful if you want SQL capabilities without setting up a full-fledged database server.\n",
    "\n",
    "# Parquet or Feather:\n",
    "# Columnar storage file formats.\n",
    "# Efficiently store large DataFrames, and they maintain data types and support fast read/write operations.\n",
    "# Integrates well with Pandas using to_parquet and read_parquet methods.\n",
    "\n",
    "# CSV Files:\n",
    "# You can save each DataFrame in tabular_document_datasets_dict as a separate CSV file in a dedicated folder for each document.\n",
    "# Human-readable and can be opened with many tools, but might be slower and less space-efficient than other formats.\n",
    "\n",
    "# Database (e.g., PostgreSQL, MySQL):\n",
    "# If you're dealing with very large datasets or need to perform complex queries, storing the data in a relational database might be beneficial.\n",
    "# Pandas can interface with databases using the to_sql and read_sql methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def draw_boxes(image_path: str, dataframe: pd.DataFrame, bbox_column: str) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Draw bounding boxes and their labels on the image.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path: Path to the image on which bounding boxes will be drawn.\n",
    "    - dataframe: DataFrame containing bounding boxes and labels.\n",
    "    - bbox_column: Column name in the dataframe which contains bounding boxes.\n",
    "    \n",
    "    Returns:\n",
    "    - Modified image with bounding boxes and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    \n",
    "    width, height = image.size\n",
    "\n",
    "    # Initialize drawing context and font\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    font = ImageFont.load_default()\n",
    "\n",
    "    dataframe = dataframe[dataframe['file_path'] == image_path]\n",
    "\n",
    "    for _, row in dataframe.iterrows():\n",
    "        # Extract bounding box and label from the dataframe\n",
    "        #print(row[bbox_column])\n",
    "        #print(type(row[bbox_column]))\n",
    "        #box = eval(row[bbox_column])  # Convert string representation of list to actual list\n",
    "        box = row[bbox_column]\n",
    "        label = row['label']\n",
    "        \n",
    "        # Draw the bounding box on the image\n",
    "        draw.rectangle(box, outline=\"red\", width=2)\n",
    "        \n",
    "        # Calculate label position (slightly above the bounding box)\n",
    "        if label != 'NO_LABEL':\n",
    "            label_position = (box[0], box[1] - 15)  # 15 pixels above the bounding box\n",
    "            draw.text(label_position, label, font=font, fill=\"blue\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "# For demonstration purposes, I won't run the function here. \n",
    "# You can call this function with the appropriate parameters to visualize the bounding boxes and labels on your image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versicherungsdokumente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project 2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "project_id = 2\n",
    "project = Project(id_=project_id, update=True, strict_data_validation=False)\n",
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_documents = project.documents\n",
    "no_status_documents = project.no_status_documents\n",
    "preparation_documents = project.preparation_documents\n",
    "test_documents = project.test_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabular_document_datasets_dict = process_all_documents(train_documents[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tabular_document_datasets_dict['axa_MIH408883794.pdf']['pages'])#['document_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "axa_MIH408883794_p2 = tabular_document_datasets_dict['axa_MIH408883794.pdf']['pages'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>page_index</th>\n",
       "      <th>line_number</th>\n",
       "      <th>offset_string</th>\n",
       "      <th>offset_string_original</th>\n",
       "      <th>top</th>\n",
       "      <th>bottom</th>\n",
       "      <th>x0</th>\n",
       "      <th>y0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_path</th>\n",
       "      <th>label</th>\n",
       "      <th>label_set</th>\n",
       "      <th>bbox</th>\n",
       "      <th>merged_bbboxes</th>\n",
       "      <th>merged_bbboxes_by_line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>1755</td>\n",
       "      <td>3</td>\n",
       "      <td>113</td>\n",
       "      <td>AXA</td>\n",
       "      <td>AXA</td>\n",
       "      <td>61.887</td>\n",
       "      <td>70.887</td>\n",
       "      <td>690.206115</td>\n",
       "      <td>107.028711</td>\n",
       "      <td>720.287900</td>\n",
       "      <td>122.593505</td>\n",
       "      <td>3127</td>\n",
       "      <td>3130</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Vertragsdetails_Versicherungsgesellschaft</td>\n",
       "      <td>Vertragsdetails</td>\n",
       "      <td>[690, 107, 720, 122]</td>\n",
       "      <td>[690, 107, 894, 122]</td>\n",
       "      <td>[690, 107, 894, 122]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>1755</td>\n",
       "      <td>3</td>\n",
       "      <td>113</td>\n",
       "      <td>Krankenversicherung</td>\n",
       "      <td>Krankenversicherung</td>\n",
       "      <td>61.887</td>\n",
       "      <td>70.887</td>\n",
       "      <td>724.949773</td>\n",
       "      <td>107.028711</td>\n",
       "      <td>868.425527</td>\n",
       "      <td>122.593505</td>\n",
       "      <td>3131</td>\n",
       "      <td>3150</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Vertragsdetails_Versicherungsgesellschaft</td>\n",
       "      <td>Vertragsdetails</td>\n",
       "      <td>[724, 107, 868, 122]</td>\n",
       "      <td>[690, 107, 894, 122]</td>\n",
       "      <td>[690, 107, 894, 122]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1755</td>\n",
       "      <td>3</td>\n",
       "      <td>113</td>\n",
       "      <td>AG</td>\n",
       "      <td>AG</td>\n",
       "      <td>61.887</td>\n",
       "      <td>70.887</td>\n",
       "      <td>873.258525</td>\n",
       "      <td>107.028711</td>\n",
       "      <td>894.867007</td>\n",
       "      <td>122.593505</td>\n",
       "      <td>3151</td>\n",
       "      <td>3153</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Vertragsdetails_Versicherungsgesellschaft</td>\n",
       "      <td>Vertragsdetails</td>\n",
       "      <td>[873, 107, 894, 122]</td>\n",
       "      <td>[690, 107, 894, 122]</td>\n",
       "      <td>[690, 107, 894, 122]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1754</td>\n",
       "      <td>3</td>\n",
       "      <td>116</td>\n",
       "      <td>4924921094</td>\n",
       "      <td>4924921094</td>\n",
       "      <td>132.380</td>\n",
       "      <td>142.380</td>\n",
       "      <td>687.440450</td>\n",
       "      <td>228.940824</td>\n",
       "      <td>791.152864</td>\n",
       "      <td>246.235040</td>\n",
       "      <td>3374</td>\n",
       "      <td>3384</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Vertragsdetails_Versicherungsnummer</td>\n",
       "      <td>Vertragsdetails</td>\n",
       "      <td>[687, 228, 791, 246]</td>\n",
       "      <td>[687, 228, 791, 246]</td>\n",
       "      <td>[687, 228, 791, 246]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>1750</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>Tamer</td>\n",
       "      <td>Tamer</td>\n",
       "      <td>168.380</td>\n",
       "      <td>178.380</td>\n",
       "      <td>386.674450</td>\n",
       "      <td>291.200000</td>\n",
       "      <td>438.530657</td>\n",
       "      <td>308.494215</td>\n",
       "      <td>3448</td>\n",
       "      <td>3453</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Kunde_Vorname</td>\n",
       "      <td>Kunde</td>\n",
       "      <td>[386, 291, 438, 308]</td>\n",
       "      <td>[386, 291, 438, 308]</td>\n",
       "      <td>[386, 291, 438, 308]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>1751</td>\n",
       "      <td>3</td>\n",
       "      <td>118</td>\n",
       "      <td>Altunakar</td>\n",
       "      <td>Altunakar</td>\n",
       "      <td>168.380</td>\n",
       "      <td>178.380</td>\n",
       "      <td>448.901898</td>\n",
       "      <td>291.200000</td>\n",
       "      <td>542.243071</td>\n",
       "      <td>308.494215</td>\n",
       "      <td>3454</td>\n",
       "      <td>3463</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Kunde_Name</td>\n",
       "      <td>Kunde</td>\n",
       "      <td>[448, 291, 542, 308]</td>\n",
       "      <td>[448, 291, 542, 308]</td>\n",
       "      <td>[448, 291, 542, 308]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>1756</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>PREMIUM</td>\n",
       "      <td>PREMIUM</td>\n",
       "      <td>324.380</td>\n",
       "      <td>334.380</td>\n",
       "      <td>127.393415</td>\n",
       "      <td>560.989761</td>\n",
       "      <td>199.992105</td>\n",
       "      <td>578.283977</td>\n",
       "      <td>4066</td>\n",
       "      <td>4073</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Einzelprodukt_Bezeichnung</td>\n",
       "      <td>Einzelprodukt</td>\n",
       "      <td>[127, 560, 199, 578]</td>\n",
       "      <td>[127, 560, 199, 578]</td>\n",
       "      <td>[127, 560, 199, 578]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>1757</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>18,97</td>\n",
       "      <td>18,97</td>\n",
       "      <td>324.380</td>\n",
       "      <td>334.380</td>\n",
       "      <td>822.266588</td>\n",
       "      <td>560.989761</td>\n",
       "      <td>874.122795</td>\n",
       "      <td>578.283977</td>\n",
       "      <td>4133</td>\n",
       "      <td>4138</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Einzelprodukt_Betrag</td>\n",
       "      <td>Einzelprodukt</td>\n",
       "      <td>[822, 560, 874, 578]</td>\n",
       "      <td>[822, 560, 874, 578]</td>\n",
       "      <td>[822, 560, 874, 578]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>1758</td>\n",
       "      <td>3</td>\n",
       "      <td>129</td>\n",
       "      <td>KOMFORT</td>\n",
       "      <td>KOMFORT</td>\n",
       "      <td>336.380</td>\n",
       "      <td>346.380</td>\n",
       "      <td>127.393415</td>\n",
       "      <td>581.742820</td>\n",
       "      <td>199.992105</td>\n",
       "      <td>599.037035</td>\n",
       "      <td>4148</td>\n",
       "      <td>4155</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Einzelprodukt_Bezeichnung</td>\n",
       "      <td>Einzelprodukt</td>\n",
       "      <td>[127, 581, 199, 599]</td>\n",
       "      <td>[127, 581, 199, 599]</td>\n",
       "      <td>[127, 581, 199, 599]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>1759</td>\n",
       "      <td>3</td>\n",
       "      <td>129</td>\n",
       "      <td>31,37</td>\n",
       "      <td>31,37</td>\n",
       "      <td>336.380</td>\n",
       "      <td>346.380</td>\n",
       "      <td>822.266588</td>\n",
       "      <td>581.742820</td>\n",
       "      <td>874.122795</td>\n",
       "      <td>599.037035</td>\n",
       "      <td>4215</td>\n",
       "      <td>4220</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Einzelprodukt_Betrag</td>\n",
       "      <td>Einzelprodukt</td>\n",
       "      <td>[822, 581, 874, 599]</td>\n",
       "      <td>[822, 581, 874, 599]</td>\n",
       "      <td>[822, 581, 874, 599]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>1761</td>\n",
       "      <td>3</td>\n",
       "      <td>131</td>\n",
       "      <td>50,34</td>\n",
       "      <td>50,34</td>\n",
       "      <td>360.380</td>\n",
       "      <td>370.380</td>\n",
       "      <td>822.266588</td>\n",
       "      <td>623.248937</td>\n",
       "      <td>874.122795</td>\n",
       "      <td>640.543152</td>\n",
       "      <td>4381</td>\n",
       "      <td>4386</td>\n",
       "      <td>page_3.png</td>\n",
       "      <td>/Users/christopherczaban/Local_Project_Files/k...</td>\n",
       "      <td>Vertragsdetails_Gesamtbruttobetrag</td>\n",
       "      <td>Vertragsdetails</td>\n",
       "      <td>[822, 623, 874, 640]</td>\n",
       "      <td>[822, 623, 874, 640]</td>\n",
       "      <td>[822, 623, 874, 640]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    annotation_id  page_index  line_number        offset_string  \\\n",
       "236          1755           3          113                  AXA   \n",
       "237          1755           3          113  Krankenversicherung   \n",
       "238          1755           3          113                   AG   \n",
       "241          1754           3          116           4924921094   \n",
       "244          1750           3          118                Tamer   \n",
       "245          1751           3          118            Altunakar   \n",
       "273          1756           3          128              PREMIUM   \n",
       "275          1757           3          128                18,97   \n",
       "276          1758           3          129              KOMFORT   \n",
       "278          1759           3          129                31,37   \n",
       "283          1761           3          131                50,34   \n",
       "\n",
       "    offset_string_original      top   bottom          x0          y0  \\\n",
       "236                    AXA   61.887   70.887  690.206115  107.028711   \n",
       "237    Krankenversicherung   61.887   70.887  724.949773  107.028711   \n",
       "238                     AG   61.887   70.887  873.258525  107.028711   \n",
       "241             4924921094  132.380  142.380  687.440450  228.940824   \n",
       "244                  Tamer  168.380  178.380  386.674450  291.200000   \n",
       "245              Altunakar  168.380  178.380  448.901898  291.200000   \n",
       "273                PREMIUM  324.380  334.380  127.393415  560.989761   \n",
       "275                  18,97  324.380  334.380  822.266588  560.989761   \n",
       "276                KOMFORT  336.380  346.380  127.393415  581.742820   \n",
       "278                  31,37  336.380  346.380  822.266588  581.742820   \n",
       "283                  50,34  360.380  370.380  822.266588  623.248937   \n",
       "\n",
       "             x1          y1  start_offset  end_offset   file_name  \\\n",
       "236  720.287900  122.593505          3127        3130  page_3.png   \n",
       "237  868.425527  122.593505          3131        3150  page_3.png   \n",
       "238  894.867007  122.593505          3151        3153  page_3.png   \n",
       "241  791.152864  246.235040          3374        3384  page_3.png   \n",
       "244  438.530657  308.494215          3448        3453  page_3.png   \n",
       "245  542.243071  308.494215          3454        3463  page_3.png   \n",
       "273  199.992105  578.283977          4066        4073  page_3.png   \n",
       "275  874.122795  578.283977          4133        4138  page_3.png   \n",
       "276  199.992105  599.037035          4148        4155  page_3.png   \n",
       "278  874.122795  599.037035          4215        4220  page_3.png   \n",
       "283  874.122795  640.543152          4381        4386  page_3.png   \n",
       "\n",
       "                                             file_path  \\\n",
       "236  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "237  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "238  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "241  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "244  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "245  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "273  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "275  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "276  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "278  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "283  /Users/christopherczaban/Local_Project_Files/k...   \n",
       "\n",
       "                                         label        label_set  \\\n",
       "236  Vertragsdetails_Versicherungsgesellschaft  Vertragsdetails   \n",
       "237  Vertragsdetails_Versicherungsgesellschaft  Vertragsdetails   \n",
       "238  Vertragsdetails_Versicherungsgesellschaft  Vertragsdetails   \n",
       "241        Vertragsdetails_Versicherungsnummer  Vertragsdetails   \n",
       "244                              Kunde_Vorname            Kunde   \n",
       "245                                 Kunde_Name            Kunde   \n",
       "273                  Einzelprodukt_Bezeichnung    Einzelprodukt   \n",
       "275                       Einzelprodukt_Betrag    Einzelprodukt   \n",
       "276                  Einzelprodukt_Bezeichnung    Einzelprodukt   \n",
       "278                       Einzelprodukt_Betrag    Einzelprodukt   \n",
       "283         Vertragsdetails_Gesamtbruttobetrag  Vertragsdetails   \n",
       "\n",
       "                     bbox        merged_bbboxes merged_bbboxes_by_line  \n",
       "236  [690, 107, 720, 122]  [690, 107, 894, 122]   [690, 107, 894, 122]  \n",
       "237  [724, 107, 868, 122]  [690, 107, 894, 122]   [690, 107, 894, 122]  \n",
       "238  [873, 107, 894, 122]  [690, 107, 894, 122]   [690, 107, 894, 122]  \n",
       "241  [687, 228, 791, 246]  [687, 228, 791, 246]   [687, 228, 791, 246]  \n",
       "244  [386, 291, 438, 308]  [386, 291, 438, 308]   [386, 291, 438, 308]  \n",
       "245  [448, 291, 542, 308]  [448, 291, 542, 308]   [448, 291, 542, 308]  \n",
       "273  [127, 560, 199, 578]  [127, 560, 199, 578]   [127, 560, 199, 578]  \n",
       "275  [822, 560, 874, 578]  [822, 560, 874, 578]   [822, 560, 874, 578]  \n",
       "276  [127, 581, 199, 599]  [127, 581, 199, 599]   [127, 581, 199, 599]  \n",
       "278  [822, 581, 874, 599]  [822, 581, 874, 599]   [822, 581, 874, 599]  \n",
       "283  [822, 623, 874, 640]  [822, 623, 874, 640]   [822, 623, 874, 640]  "
      ]
     },
     "execution_count": 753,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axa_MIH408883794_p2[axa_MIH408883794_p2['label'] != 'NO_LABEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_json(tabular_document_datasets_dict, 'konfuzio_converted_insurance_dataset.json')\n",
    "\n",
    "\n",
    "file_name = 'page_1.png'\n",
    "file_path = full_df[full_df['file_name'] == file_name]['file_path'].iloc[0]\n",
    "print(file_path)\n",
    "bboxes = full_df[full_df['file_name'] == file_name]['bbox']\n",
    "\n",
    "draw_boxes(file_path, full_df, 'merged_bbboxes_by_line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
